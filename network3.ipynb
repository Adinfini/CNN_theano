{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda install -c conda-forge theano\n",
    "#%pip install numpy\n",
    "#%pip install scipy\n",
    "#%pip install theano\n",
    "\n",
    "import _pickle as cPickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "#import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import conv\n",
    "from theano.tensor.nnet import softmax\n",
    "from theano.tensor import shared_randomstreams\n",
    "from theano.tensor.signal.pool import pool_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activations functions for neurons\n",
    "def linear(z):\n",
    "    return z\n",
    "\n",
    "def RelU(z):\n",
    "    return T.maximum(0.0, z)\n",
    "\n",
    "from theano.tensor.nnet import sigmoid\n",
    "from theano.tensor import tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to run under a GPU.  If this is not desired, then modify network3.py\n",
      "to set the GPU flag to False.\n"
     ]
    }
   ],
   "source": [
    "#### Constants\n",
    "GPU = True\n",
    "if GPU:\n",
    "    print(\"Trying to run under a GPU.  If this is not desired, then modify \"+\\\n",
    "        \"network3.py\\nto set the GPU flag to False.\")\n",
    "    try: theano.config.device = 'gpu'\n",
    "    except: pass # it's already set\n",
    "    theano.config.floatX = 'float32'\n",
    "else:\n",
    "    print(\"Running with a CPU.  If this is not desired, then the modify \"+\\\n",
    "        \"network3.py to set\\nthe GPU flag to True.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load Mnist data\n",
    "def load_data_shared(filename=\"data/mnist.pkl.gz\"):\n",
    "    f = gzip.open(filename, 'rb')\n",
    "\n",
    "    training_data, validation_data, test_data= cPickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    def shared(data):\n",
    "        \"\"\"Place the data into shared variables.This allows Theano to copy\n",
    "        the data to the GPU, if one is available.\"\"\"\n",
    "        shared_x = theano.shared(\n",
    "            np.asarray(data[0], dtype=theano.config.floatX), borrow=True)\n",
    "        shared_y = theano.shared(np.asarray(data[1], dtype=theano.config.floatX), borrow=True)\n",
    "        return shared_x, T.cast(shared_y, \"int32\")\n",
    "    return[shared(training_data), shared(validation_data), shared(test_data)]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscellanea\n",
    "def size(data):\n",
    "    \"Return the size of the dataset `data`.\"\n",
    "    return data[0].get_value(borrow=True).shape[0]\n",
    "\n",
    "def dropout_layer(layer, p_dropout):\n",
    "    srng = shared_randomstreams.RandomStreams(np.random.RandomState(0).randint(999999))\n",
    "    mask = srng.binomial(n=1, p=1-p_dropout, size=layer.shape)\n",
    "    return layer*T.cast(mask, theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(object):\n",
    "    def __init__(self, n_in, n_out, activation_fn=sigmoid, p_dropout=0.0) :\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.activation_fn = activation_fn\n",
    "        self.p_dropout = p_dropout\n",
    "        #Initialize weights and biases\n",
    "        self.w = theano.shared(\n",
    "            np.asarray(\n",
    "                np.random.normal(\n",
    "                    loc=0.0, scale=np.sqrt(1.0/n_out), size=(n_in, n_out)),\n",
    "            dtype=theano.config.floatX),\n",
    "            name='w', borrow=True) \n",
    "        self.b = theano.shared(\n",
    "            np.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out,)),\n",
    "                       dtype=theano.config.floatX),\n",
    "            name='b', borrow=True)\n",
    "        self.params = [self.w, self.b]\n",
    "\n",
    "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
    "        self.inpt = inpt.reshape(mini_batch_size, self.n_in)\n",
    "        self.output = self.activation_fn(\n",
    "            (1-self.p_dropout)* T.dot(self.inpt, axis=1) + self.b)\n",
    "        self.y_out = T.argmax(self.output, axis=1)\n",
    "        self.inpt_dropout = dropout_layer(inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
    "        self.output_dropout = self.activation_fn((T.dot(self.inpt_dropout, self.w)) + self.b)\n",
    "\n",
    "    def accuracy(self, y):\n",
    "        \"Return the accuracy for the mini_batch\"\n",
    "        return T.mean(T.eq(y, self.y_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, layers, mini_batch_size):\n",
    "        \"\"\"Takes a list of  'layers', describing the network architecture, and\n",
    "        a value for the 'mini_batch_size' to be used  during training by stochastic gradient descent.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.params = [param for layer in self.layers for param in layers.params]\n",
    "        self.x = T.matrix(\"x\")\n",
    "        self.y = T.ivector(\"y\")\n",
    "        init_layer = self.layers[0]\n",
    "        init_layer.set_inpt(self.x, self.x, self.mini_batch_size)\n",
    "        for j in range(1, len(self.layers)):\n",
    "            prev_layer, layer = self.layers[j-1], self.layers[j]\n",
    "            layers.set_inpt(prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)\n",
    "        self.output = self.layers[-1].output\n",
    "        self.output_dropout = self.layers[-1].output_dropout     \n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, validation_data,\n",
    "            test_data, lmda=0.0):\n",
    "        \"\"\"Train the network using mini_batch stochastic gradient descent\"\"\"\n",
    "        training_x , training_y = training_data\n",
    "        validation_x, validation_y = validation_data\n",
    "        test_x, test_y = test_data\n",
    "\n",
    "\n",
    "        #compute number of minibatches for training, valditaion and testing\n",
    "        num_training_batches = size(training_data)/mini_batch_size\n",
    "        num_validation_batches = size(validation_data)/mini_batch_size\n",
    "        num_test_batches = size(test_data)/mini_batch_size\n",
    "\n",
    "\n",
    "        #define the (regularized) cost function, symbolic gradients, and updates\n",
    "        l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers])\n",
    "        cost = self.layers[-1].cost(self)+ 0.5*lmda*l2_norm_squared/num_training_batches\n",
    "        grads = T.grad(cost, self.params)\n",
    "        updates =  [(param, param-eta*grad) for param, grad in zip(self.params, grads)]\n",
    "\n",
    "        #define a function to train a mini_batch , and to compute the\n",
    "        # accuracy in validation and test mini_batches\n",
    "\n",
    "        i = T.scalar()#mini_batch_index\n",
    "        train_mb = theano.function(\n",
    "            [i], cost, updates=updates,\n",
    "            givens={    \n",
    "                self.x:\n",
    "                training_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
    "                self.y:\n",
    "                training_y[i*self.mini_batch_size:(i+1)*self.mini_batch_size]\n",
    "            })\n",
    "        validate_mb_accuracy = theano.function(\n",
    "            [i], self.layers[-1].accuracy(self.y),\n",
    "            givens={\n",
    "                self.x:\n",
    "                test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
    "                self.y:\n",
    "                test_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
    "            })\n",
    "        test_mb_accuracy = theano.function(\n",
    "            [i], self.layers[-1].accuracy(self.y),\n",
    "            givens={\n",
    "                self.x:\n",
    "                test_x[i*self.mini_batch_size:(i+1)*self.mini_batch_size],\n",
    "                self.y:\n",
    "                test_y[i*self.mini_batch_size:(i+1)*self.mini_batch_size]\n",
    "            })\n",
    "        self.test_mb_predictions = theano.function(\n",
    "            [i], self.layers[-1].y_out, \n",
    "            givens={\n",
    "                self.x:\n",
    "                test_x[i*self.mini_batch_size:(i+1)*self.mini_batch_size]\n",
    "            })\n",
    "        #Do the acutal training\n",
    "        best_validation_accuracy = 0.0\n",
    "        for epoch in range(epochs):\n",
    "            for minibatch_index in range(num_training_batches):\n",
    "                iteration = num_training_batches*epoch + minibatch_index\n",
    "                if iteration % 1000 == 0:\n",
    "                    print(\"Training mini_batch_number {0}\".format(iteration))\n",
    "                cost_ij = train_mb(minibatch_index)\n",
    "                if(iteration + 1) % num_training_batches == 0:\n",
    "                    validation_accuracy = np.mean(\n",
    "                        [validate_mb_accuracy(j) for j in range(num_validation_batches)])        \n",
    "                    print(\"Epoch {0}:validation accuracy{1:.2%}\".format(epoch, validation_accuracy))\n",
    "                if validation_accuracy >= best_validation_accuracy:\n",
    "                    print(\"This is the best validation accuracy to date\")\n",
    "                    best_validation_accuracy = validation_accuracy\n",
    "                    best_iteration = iteration\n",
    "                    if test_data:\n",
    "                        test_accuracy = np.mean([test_mb_accuracy(j) for j in range(num_test_batches)])\n",
    "                        print(\"The corresponding test accuracy is {0:.2%}\".format(test_accuracy))\n",
    "        print(\"Finished training network\")\n",
    "        print(\"Best validation accuracy of {0:.2%}\".format(best_validation_accuracy, best_iteration))\n",
    "        print(\"Corresponding test accuracy of \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define layer types\n",
    "class ConvPooLayer(object):\n",
    "    \"\"\" Used to create a combination of convolutional and a max-pooling layer.\n",
    "    A more sophisticated implementation would separate the two, but for our purpose \n",
    "    we'll always use them together, and it simplifies the code, so it makes sense to combine them.\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_shape, image_shape, poolsize=(2, 2),\n",
    "                 activation_fn= sigmoid):\n",
    "        \"\"\" 'filter shape ' is a tuple of length 4, whose entries are the number \n",
    "        of filters, the number of input feature maps, the filter height, and the\n",
    "        filter width.\n",
    "\n",
    "        'image shape'  is a tuple of length 4, whose entries are the mini-batch size, the number of input feature maps,\n",
    "        the image height, and the image width.\n",
    "\n",
    "        'poolsize' is a tuple of length 2, whose entries are the y and x pooling sizes. \n",
    "        \"\"\"\n",
    "        self.filter_shape = filter_shape\n",
    "        self.image_shape = image_shape\n",
    "        self.poolsize = poolsize\n",
    "        self.activation_fn = activation_fn\n",
    "        #initialize weights and biases\n",
    "        n_out = (filter_shape[0]*np.prod(filter_shape[2:])/np.prod(poolsize))\n",
    "        self.w = theano.shared(\n",
    "            np.asanyarray(\n",
    "                np.random.normal(loc=0, scale=np.sqrt(1.0/n_out), size=filter_shape),\n",
    "                dtype=theano.config.floatX), borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            np.asarray(\n",
    "                np.random.normal(loc=0, scale=1.0, size=(filter_shape[0],)),\n",
    "                dtype=theano.config.floatX), borrow=True)\n",
    "        self.params = [self.w, self.b]\n",
    "\n",
    "\n",
    "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
    "        self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
    "        self.output = self.activation_fn((1-self.p_dropout)*T.dot(self.inpt, self.w)+ self.b)\n",
    "        self.y_out = T.argmax(self.output, axis=1)\n",
    "        self.inpt_dropout = dropout_layer(inpt_dropout.reshape(mini_batch_size, self.n_in), self.p_dropout)\n",
    "        self.output_dropout = self.activation_fn(T.dot(self.inpt_dropout, self.w) + self.b)\n",
    "\n",
    "    def accuracy(self, w):\n",
    "        \"Return the accuracy for the mini_batch\"\n",
    "        return T.mean(T.eq(y, self.y_out))\n",
    "    \n",
    "\n",
    "class SoftmaxLayer(object):\n",
    "\n",
    "    def __init__(self, n_in, n_out, p_dropout=0.0):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.p_dropout = p_dropout\n",
    "        # Initialize weights and biases\n",
    "        self.w = theano.shared(\n",
    "            np.zeros((n_in, n_out), dtype=theano.config.floatX),\n",
    "            name='w', borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            np.zeros((n_out,), dtype=theano.config.floatX),\n",
    "            name='b', borrow=True)\n",
    "        self.params = [self.w, self.b]\n",
    "\n",
    "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
    "        self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
    "        self.output = softmax((1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
    "        self.y_out = T.argmax(self.output, axis=1)\n",
    "        self.inpt_dropout = dropout_layer(\n",
    "            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
    "        self.output_dropout = softmax(T.dot(self.inpt_dropout, self.w) + self.b)\n",
    "\n",
    "    def cost(self, net):\n",
    "        \"Return the log-likelihood cost.\"\n",
    "        return -T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[0]), net.y])\n",
    "\n",
    "    def accuracy(self, y):\n",
    "        \"Return the accuracy for the mini-batch.\"\n",
    "        return T.mean(T.eq(y, self.y_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and testing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
